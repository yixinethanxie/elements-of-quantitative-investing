{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de72f1-1806-4ac1-9dec-51e42f064177",
   "metadata": {},
   "source": [
    "# Chapter 4 Appendix\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "### Loss function\n",
    "\n",
    "$$\\min_{\\hat{y}} E\\left[(\\hat{y}(x) - y)^2 | x\\right]$$\n",
    "\n",
    "Theory: if $E(y^2|x) < \\infty$, then the minimizing function $E(y|x)$\n",
    "\n",
    "Proof: Let\n",
    "$$y = E(y|x) + \\epsilon$$\n",
    "$$\\epsilon = y - E(y|x)$$\n",
    "$$E(\\epsilon^2|x) = E((y-E(y|x))^2|x)$$\n",
    "$$E\\left[(\\hat{y}(x) - y)^2\\right|x]=E(\\epsilon^2|x) + E\\left[(\\hat{y} - E(y|x))^2|x\\right] \\geq E(\\epsilon^2|x)$$\n",
    "\n",
    "The equality holds if $E\\left[(\\hat{y} - E(y|x))^2|x\\right]=0$, $\\hat{y} = E(y|x)$. Next\n",
    "\n",
    "$$E(\\epsilon^2|x) = E\\left[(y - E(y|x))^2|x\\right] = E(y^2|x) - 2E(yE(y|x)|x) + E(E(y|x)^2|x)$$\n",
    "\n",
    "Since\n",
    "$$E((y + E(y|x))^2|x) = E(y^2|x) + 2E(yE(y|x)) + E(E(y|x)^2|x) = E(y^2|x) + 2E(y|x)^2 + E(E(y|x)^2|x) \\geq 0$$\n",
    "$$-2E(y|x)^2 \\leq E(y^2|x) + E(E(y|x)^2|x)$$\n",
    "\n",
    "Plug back in the original\n",
    "\n",
    "$$E(\\epsilon^2|x) \\leq 2E(y^2|x) + 2E(E(y|x)^2|x)$$\n",
    "\n",
    "Since $f(x) = x^2$ is convex, Jensen's inequality\n",
    "\n",
    "$$f(E(y|x)) \\leq E(f(y)|x)$$\n",
    "$$E(y|x)^2 \\leq E(y^2|x)$$\n",
    "\n",
    "Plug back in the original\n",
    "\n",
    "$$E(\\epsilon^2|x) \\leq 2E(y^2|x) + 2E(E(y^2|x)|x) = 2E(y^2|x) + 2E(y^2|x) = 4E(y^2|x) \\lt \\infty$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Three ways to view optimization:\n",
    "- Select $\\beta$ to minimize the loss function: loss function is convex, so set the first order derivative of $y$ w.r.t $X$ to 0 and solve for $\\beta$\n",
    "- Maximum likelihood function: since the error term is $\\epsilon$ is standard normal, fit $\\beta$ such that the cumulative likelihood accross all data point is maximized\n",
    "- Projection: from linear algebra lens, $\\hat{y}$ is $y$'s projection on the subspace where $X$ is a basis.\n",
    "\n",
    "### Variances\n",
    "\n",
    "Variance of estimated $\\beta$\n",
    "\n",
    "$$var(\\hat{\\beta}) = var\\left[(X^TX)^{-1}X^T(X\\beta+\\epsilon)\\right]=var\\left[(X^TX)^{-1}X^T\\epsilon\\right]$$\n",
    "\n",
    "Since $var(\\epsilon) = \\sigma^2 I$$\n",
    "\n",
    "$$var(\\hat{\\beta}) = (X^TX)^{-1}X^Tvar(\\epsilon)((X^TX)^{-1}X^T)^T=\\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}\n",
    "=\\sigma^2(X^TX)^{-1}$$\n",
    "\n",
    "Similarly \n",
    "\n",
    "$$var(\\hat{y}) = var(X\\hat{\\beta}) = Xvar(\\hat{\\beta})X^T = \\sigma^2X(X^TX)^{-1}X^T$$\n",
    "\n",
    "By SVD, $X = U\\Lambda V^T$\n",
    "\n",
    "$$var(\\hat{\\beta})=\\sigma^2(V\\Lambda U^T U\\Lambda V^T)^{-1} = \\sigma^2 V \\Lambda^{-2} V^T$$\n",
    "\n",
    "As the columns in $X$ become more collinear, the $\\hat{\\beta}$ gets larger. To see this, we first solve exercise 4.2: If a matrix $X$ has near collinear columns,then there is a unit-form vector $u$ such that $||Xu||^2=h$ for some small positive h.\n",
    "\n",
    "1. Show that $u^T(X^TX)u = h$.\n",
    "$$||Xu||^2 = (u^TX^T)Xu = u^T(X^TX)u = h$$\n",
    "2. Let $\\lambda_i$ be the eigenvalues of $X^TX$. Show that $min_i{\\lambda_i}^2 \\leq h^2$ (see errata).\n",
    "$$X^TX = V\\Lambda V^T$$\n",
    "$$(X^TX)^2 = V\\Lambda^2 V^T$$\n",
    "$$(u^TX^TXu)^2 = u^T(X^TX)^2u = h^2$$\n",
    "By Rayleigh Ritz theorem\n",
    "$${\\lambda_1}^2 \\leq h^2 \\leq {\\lambda_m}^2$$\n",
    "where the eigenvalues of $(X^TX)^2$ are\n",
    "$${\\lambda_1}^2 \\leq  {\\lambda_2}^2 \\ldots \\leq {\\lambda_m}^2$$\n",
    "3. From this, show that $\\sum_i var(\\hat{\\beta_i})^2 \\geq \\max_i {\\lambda_i}^{-2} \\geq 1/h^2 = ||Xu||^{-2}$\n",
    "\n",
    "$$var(\\hat{\\beta}) = \\sigma^2(X^TX)^{-1} = \\sigma^2V\\Lambda^{-1}V^T$$\n",
    "$$\\sum_i var(\\hat{\\beta_i})^2 = \\sigma^4Tr((X^TX)^{-2}) = \\sigma^4Tr(\\Lambda^{-2}) = \\sigma^4\\sum_i {\\lambda_i}^{-2} \\geq \\sigma^4 \\lambda_1^{-2} \\geq \\sigma^4/h^2 = \\sigma^4 ||Xu||^{-4}$$\n",
    "\n",
    "The first inequality is because the eignvalues of a positive semidefinite matrix is always non-negative. If a matrix has collinearity, there is at least one column that is adding little to no additional information orthogonal to the rest, therefore the min eigenvalue will be very small compared to the other eigenvalues (high condition number). Based on the inequality derived above, the variance of the estimated coefficients $\\hat{\\beta}$ is bounded below by $\\lambda_1^{-2}$. If $\\lambda_1$ is too small, the variance of $\\hat{\\beta}$ will explode, rendering the estimation unstable.\n",
    "\n",
    "### Linear Regression Decomposition\n",
    "\n",
    "The main idea is that, when adding an additional columns to matrix $X$, we don't have to re-fit the entire model. We can run three regressions:\n",
    "- regression 1: regress $y$ on $X$, which is already done, and get the residual vector $\\epsilon$\n",
    "- regression 2: regress the additional matrix $X_1$ on the original matrix $X$, and get the residual matrix $\\tilde{X}_1$\n",
    "  - note that ${\\tilde{X}_1}^TX = 0$, otherwise, the model should be modified since part of the residual can still be explained by $X$\n",
    "- regression 3: regress the residual from step 1 $\\epsilon$ to the residual from step 2 $\\tilde{X}_1$ and get estimated coefficient vector $\\hat{\\beta_1}$\n",
    "\n",
    "The original model is:\n",
    "$$y = \\hat{\\beta}X$$\n",
    "\n",
    "The new model with the additional matrix is:\n",
    "$$y = \\hat{\\beta}X + \\hat{\\beta_1}\\tilde{X}_1$$\n",
    "\n",
    "What's interesting is that we didn't have to fit the entire model again to get $\\hat{\\beta_1}$. Because we are using the orthogonal component of $X_1$ to explain the residuals from the original model $\\epsilon$, which should not interfere with the original model.\n",
    "\n",
    "### Singular value decomposition (SVD)\n",
    "\n",
    "SVD is almost half of EVD (eigenvalue decomposition), because EVD is usually applied to quadratric matrix ($A^TA$) and SVD deals with $A$:\n",
    "$$A^TA = V\\Lambda V^T$$\n",
    "$$(A^TA)v_i = \\lambda_i v_i$$\n",
    "$$A = USV^T$$\n",
    "$$Av_i = s_iu_i$$\n",
    "\n",
    "Define\n",
    "$$s_i = \\sqrt{\\lambda_i}$$\n",
    "$$u_i = (Av_i)/s_i$$\n",
    "\n",
    "Then\n",
    "$${u_i}^Tu_j = \\frac{{v_i}^T(A^TAv_j)}{s_is_j} = \\frac{{s_j}^2{v_i}^Tv_j}{s_is_j} = \\frac{s_i}{s_j}{v_i}^Tv_j = \\delta_{i,j}$$\n",
    "\n",
    "Since $r \\leq \\min(m, n)$, fill the rest of $V$ and $U$ with orthonormal vectors such that $Av_i=0$, and fill the rest of $S$ with 0.\n",
    "\n",
    "Key insights:\n",
    "- first r columns of $U$ and $V$ are uniquely determined if and only if signular values are unique\n",
    "- power of $A$ can be computed by directly to $S$: $A^a = US^aV^T = Udiag({s_1}^a, ...,{s_n}^a)V^T$\n",
    "- SVD decomposes a matrix into a sum of rank-one matrices: $A = \\sum_{i=1}^{r}s_iu_i{v_i}^T$\n",
    "- $A^TA$ and $AA^T$ have the same eigenvalues\n",
    "- $Ax=USV^Tx$ applies a series of actions\n",
    "  - rotate $x$ on a ball with the othonormal $V^T$\n",
    "  - reshape the ball $V^Tx$ with $S$ into an ellipsoid\n",
    "  - rotate the ellipsoid $SV^Tx$ with $U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fcf12-7e3d-4a43-8b67-a574c31137dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
