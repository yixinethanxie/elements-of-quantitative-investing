{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de72f1-1806-4ac1-9dec-51e42f064177",
   "metadata": {},
   "source": [
    "# Chapter 4 Appendix\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "### Loss function\n",
    "\n",
    "$$\\min_{\\hat{y}} E\\left[(\\hat{y}(x) - y)^2 | x\\right]$$\n",
    "\n",
    "Theory: if $E(y^2|x) < \\infty$, then the minimizing function $E(y|x)$\n",
    "\n",
    "Proof: Let\n",
    "$$y = E(y|x) + \\epsilon$$\n",
    "$$\\epsilon = y - E(y|x)$$\n",
    "$$E(\\epsilon^2|x) = E((y-E(y|x))^2|x)$$\n",
    "$$E\\left[(\\hat{y}(x) - y)^2\\right|x]=E(\\epsilon^2|x) + E\\left[(\\hat{y} - E(y|x))^2|x\\right] \\geq E(\\epsilon^2|x)$$\n",
    "\n",
    "The equality holds if $E\\left[(\\hat{y} - E(y|x))^2|x\\right]=0$, $\\hat{y} = E(y|x)$. Next\n",
    "\n",
    "$$E(\\epsilon^2|x) = E\\left[(y - E(y|x))^2|x\\right] = E(y^2|x) - 2E(yE(y|x)|x) + E(E(y|x)^2|x)$$\n",
    "\n",
    "Since\n",
    "$$E((y + E(y|x))^2|x) = E(y^2|x) + 2E(yE(y|x)) + E(E(y|x)^2|x) = E(y^2|x) + 2E(y|x)^2 + E(E(y|x)^2|x) \\geq 0$$\n",
    "$$-2E(y|x)^2 \\leq E(y^2|x) + E(E(y|x)^2|x)$$\n",
    "\n",
    "Plug back in the original\n",
    "\n",
    "$$E(\\epsilon^2|x) \\leq 2E(y^2|x) + 2E(E(y|x)^2|x)$$\n",
    "\n",
    "Since $f(x) = x^2$ is convex, Jensen's inequality\n",
    "\n",
    "$$f(E(y|x)) \\leq E(f(y)|x)$$\n",
    "$$E(y|x)^2 \\leq E(y^2|x)$$\n",
    "\n",
    "Plug back in the original\n",
    "\n",
    "$$E(\\epsilon^2|x) \\leq 2E(y^2|x) + 2E(E(y^2|x)|x) = 2E(y^2|x) + 2E(y^2|x) = 4E(y^2|x) \\lt \\infty$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Three ways to view optimization:\n",
    "- Select $\\beta$ to minimize the loss function: loss function is convex, so set the first order derivative of $y$ w.r.t $X$ to 0 and solve for $\\beta$\n",
    "- Maximum likelihood function: since the error term is $\\epsilon$ is standard normal, fit $\\beta$ such that the cumulative likelihood accross all data point is maximized\n",
    "- Projection: from linear algebra lens, $\\hat{y}$ is $y$'s projection on the subspace where $X$ is a basis.\n",
    "\n",
    "### Variances\n",
    "\n",
    "Variance of estimated $\\beta$\n",
    "\n",
    "$$var(\\hat{\\beta}) = var\\left[(X^TX)^{-1}X^T(X\\beta+\\epsilon)\\right]=var\\left[(X^TX)^{-1}X^T\\epsilon\\right]$$\n",
    "\n",
    "Since $var(\\epsilon) = \\sigma^2 I$$\n",
    "\n",
    "$$var(\\hat{\\beta}) = (X^TX)^{-1}X^Tvar(\\epsilon)((X^TX)^{-1}X^T)^T=\\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}\n",
    "=\\sigma^2(X^TX)^{-1}$$\n",
    "\n",
    "Similarly \n",
    "\n",
    "$$var(\\hat{y}) = var(X\\hat{\\beta}) = Xvar(\\hat{\\beta})X^T = \\sigma^2X(X^TX)^{-1}X^T$$\n",
    "\n",
    "By SVD, $X = U\\Lambda V^T$\n",
    "\n",
    "$$var(\\hat{\\beta})=\\sigma^2(V\\Lambda U^T U\\Lambda V^T)^{-1} = \\sigma^2 V \\Lambda^{-2} V^T$$\n",
    "\n",
    "As the columns in $X$ become more collinear, the $\\hat{\\beta}$ gets larger. To see this, we first solve exercise 4.2: If a matrix $X$ has near collinear columns,then there is a unit-form vector $u$ such that $||Xu||^2=h$ for some small positive h.\n",
    "\n",
    "1. Show that $u^T(X^TX)u = h$.\n",
    "$$||Xu||^2 = (u^TX^T)Xu = u^T(X^TX)u = h$$\n",
    "2. Let $\\lambda_i$ be the eigenvalues of $X^TX$. Show that $min_i{\\lambda_i}^2 \\leq h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fcf12-7e3d-4a43-8b67-a574c31137dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
