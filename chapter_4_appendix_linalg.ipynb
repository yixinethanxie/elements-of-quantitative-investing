{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de72f1-1806-4ac1-9dec-51e42f064177",
   "metadata": {},
   "source": [
    "# Chapter 4 Appendix\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "### Loss function\n",
    "\n",
    "$$\\min_{\\hat{y}} E\\left[(\\hat{y}(x) - y)^2 | x\\right]$$\n",
    "\n",
    "Theory: if $E(y^2|x) < \\infty$, then the minimizing function $E(y|x)$\n",
    "\n",
    "Proof: Let\n",
    "$$y = E(y|x) + \\epsilon$$\n",
    "$$\\epsilon = y - E(y|x)$$\n",
    "$$E(\\epsilon^2|x) = E((y-E(y|x))^2|x)$$\n",
    "$$E\\left[(\\hat{y}(x) - y)^2\\right|x]=E(\\epsilon^2|x) + E\\left[(\\hat{y} - E(y|x))^2|x\\right] \\geq E(\\epsilon^2|x)$$\n",
    "\n",
    "The equality holds if $E\\left[(\\hat{y} - E(y|x))^2|x\\right]=0$, $\\hat{y} = E(y|x)$. Next\n",
    "\n",
    "$$E(\\epsilon^2|x) = E\\left[(y - E(y|x))^2|x\\right] = E(y^2|x) - 2E(yE(y|x)|x) + E(E(y|x)^2|x)$$\n",
    "\n",
    "Since\n",
    "$$E((y + E(y|x))^2|x) = E(y^2|x) + 2E(yE(y|x)) + E(E(y|x)^2|x) = E(y^2|x) + 2E(y|x)^2 + E(E(y|x)^2|x) \\geq 0$$\n",
    "$$-2E(y|x)^2 \\leq E(y^2|x) + E(E(y|x)^2|x)$$\n",
    "\n",
    "Plug back in the original\n",
    "\n",
    "$$E(\\epsilon^2|x) \\leq 2E(y^2|x) + 2E(E(y|x)^2|x)$$\n",
    "\n",
    "Since $f(x) = x^2$ is convex, Jensen's inequality\n",
    "\n",
    "$$f(E(y|x)) \\leq E(f(y)|x)$$\n",
    "$$E(y|x)^2 \\leq E(y^2|x)$$\n",
    "\n",
    "Plug back in the original\n",
    "\n",
    "$$E(\\epsilon^2|x) \\leq 2E(y^2|x) + 2E(E(y^2|x)|x) = 2E(y^2|x) + 2E(y^2|x) = 4E(y^2|x) \\lt \\infty$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Three ways to view optimization:\n",
    "- Select $\\beta$ to minimize the loss function: loss function is convex, so set the first order derivative of $y$ w.r.t $X$ to 0 and solve for $\\beta$\n",
    "- Maximum likelihood function: since the error term is $\\epsilon$ is standard normal, fit $\\beta$ such that the cumulative likelihood accross all data point is maximized\n",
    "- Projection: from linear algebra lens, $\\hat{y}$ is $y$'s projection on the subspace where $X$ is a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fcf12-7e3d-4a43-8b67-a574c31137dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
