{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fe1aa3-0477-4f5b-bbc3-708cddacb0ac",
   "metadata": {},
   "source": [
    "# Fundamental Factor Models\n",
    "\n",
    "Fundamental factor models start with asset returns $r_t$ and factor loadings $B_t$, and estimates factor returns $f_t$ and idio returns $\\epsilon_t$\n",
    "\n",
    "Process:\n",
    "- Data ingestion: correctness, outliers, consistency across vendors, missing data\n",
    "- Estimation universe selection\n",
    "- Winsorization: identify outliers and winsorize\n",
    "- Loading generation: generate $B_t$\n",
    "- Cross-sectional regression: estimate $f_t$ and $\\epsilon_t$\n",
    "- Time-series estimation:\n",
    "  - factor cov matrix\n",
    "  - idio cov matrix\n",
    "  - risk-adjusted performance of factor returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301b924-4dc4-49c8-9e54-8b5f9d1166b1",
   "metadata": {},
   "source": [
    "## Cross-sectional regression\n",
    "\n",
    "Starting with a single period model:\n",
    "\n",
    "$$ r_t = Bf_t + \\epsilon_t $$\n",
    "\n",
    "Where $r_t$ and $f_t$ are column vectors. To estimate $f_t$, we solve the following optimization problem:\n",
    "\n",
    "$$ \\min ||r_t - Bf_t||^2 $$\n",
    "$$ \\text{s.t. } f \\in R^{m}$$\n",
    "\n",
    "However, since the idio returns $\\epsilon_t$ usually aren't homoskedastic (same variance), we need to make them so:\n",
    "\n",
    "$$ \\Omega_{\\epsilon}^{-1/2}r_t = \\Omega_{\\epsilon}^{-1/2}Bf_t + \\Omega_{\\epsilon}^{-1/2}\\epsilon_t $$\n",
    "$$ var(\\Omega_{\\epsilon}^{-1/2}\\epsilon_t) = \n",
    "    \\Omega_{\\epsilon}^{-1/2}\\Omega_{\\epsilon}\\Omega_{\\epsilon}^{-1/2} = I_m\n",
    "$$\n",
    "\n",
    "And the optimization problem becomes\n",
    "\n",
    "$$ \\min ||\\Omega_{\\epsilon}^{-1/2}(r_t - Bf_t)||^2 $$\n",
    "$$ \\text{s.t. } f \\in R^{m}$$\n",
    "\n",
    "The solution is the normal equation: \n",
    "\n",
    "$$ \\hat{f}_t = (B^T\\Omega_{\\epsilon}^{-1}B)^{-1}B^T\\Omega_{\\epsilon}^{-1}r_t $$\n",
    "\n",
    "For multi-period case where the factor returns $F \\in R^{mxT}$, the method stays the same as each single-period factor returns can be solved independently and combined to get the factor returns matrix $F$\n",
    "\n",
    "### Idio covariance matrix\n",
    "\n",
    "We don't know idio cov matrix before solving for factor returns, and we need idio cov matrix to solve for factor returns. It's a chicken and egg problem. We can borrow from Expectation-Maximization procedure, \n",
    "- we start with an identity idio cov matrix\n",
    "- solve for factor returns and get a new idio returns and cov matrix\n",
    "- solve for factor returns again using the new idio returns, get new factor returns and update the idio cov matrix.\n",
    "\n",
    "If we have $T$ time stamps, we can use the first half of the samples to estimate idio cov matrix, then use and update it in a walk-forward manner\n",
    "- Use the idio cov matrix from time $t-1$ to estimate factor returns and idio returns at time $t$\n",
    "- Update idio cov matrix with the new idio returns from time $t$ \n",
    "\n",
    "### Rank-deficient Loading Matrices\n",
    "\n",
    "An example would be, a country factor and an industry factor, each asset must belong to one and only one country and one industry. Therefore, the sum of all country loadings equals the sum of all industry loadings equals a vector of ones. A vector that assign 1 to all country loadings and -1 to all industry loadings and 0 everywhere else will create a 0 vector, thus the loading matrix is rank-deficient due to non-empty null space. If $B$ is rank-deficient, so is $B^TB$ and we can't inverse it, therefore we can't get the factor returns estimate $\\hat{f}_t$.\n",
    "\n",
    "Ways to deal with this:\n",
    "- Add ridge regularization: $||r-Bf||^2 + \\delta ||f||^2$. As $\\delta \\to 0$, this becomes ridgeless, and we can calculate the peudo-inverse of $B^TB$ for factor returns estimate.\n",
    "- Or, we can add constraint, for example: $w^TB_{ind}f_{ind}=0$, where $w$ is the firm's market cap weight, which indicates that the market-weighted industry returns must be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2bf859-0743-44a0-8c8e-ae262749c50c",
   "metadata": {},
   "source": [
    "## Estimating Factor Covariance Matrix\n",
    "\n",
    "$$\\Omega_T^{emp} = T^{-1}\\sum^T_{t=1}\\hat{f}_t\\hat{f}_t^T$$\n",
    "The standard error is $\\sqrt{2/T}$, so as $T \\to \\infty$, $\\Omega_T^{emp} \\to \\Omega_f$\n",
    "\n",
    "The issues are:\n",
    "- The number of factor is not much smaller than the number of observations, then the estimate is noisy\n",
    "- Factor return estimate is inflated by the estimation process\n",
    "- Factor returns are non-stationary during crisis\n",
    "- Factor returns are mildy autocorrelated\n",
    "\n",
    "### Factor covariance matrix shrinkage\n",
    "\n",
    "The estimate $var(\\hat{f}_t)$ is biased:\n",
    "\n",
    "$$var(\\hat{f}_t) = \\Omega_f + (B^T\\Omega_{\\epsilon}^{-1}B)^{-1}$$\n",
    "\n",
    "**WHY?????**\n",
    "\n",
    "To correct this, we set\n",
    "$$ \\hat{\\Omega}_f = var(\\hat{f}_t) - (B^T\\Omega_{\\epsilon}^{-1}B)^{-1}$$\n",
    "\n",
    "Combined with the Ledoit-Wolf shrinkage\n",
    "\n",
    "$$ \\Omega_{f, shrink}(\\rho) = (1-\\rho)\\hat{\\Omega}_f + \\rho\\frac{trace(\\hat{\\Omega}_f)}{m}I_m$$\n",
    "\n",
    "### Dynamic Conditional Correlation\n",
    "\n",
    "The idea is to estimate the correlation matrix and factor volatilities separately using EMA, where correlation matrix changes slowly while factor vols change fast:\n",
    "\n",
    "$$\\Omega_f = VCV$$\n",
    "$$diag(V^2) = \\kappa_V\\sum^T_{s=0}e^{-s/\\tau_V}\\hat{f}_{t-s} \\circ \\hat{f}_{t-s}$$\n",
    "$$C = \\kappa_C\\sum^T_{s=0}e^{-s/\\tau_C}V_{t-s}^{-1}\\hat{f}_{t-s}\\hat{f}_{t-s}^TV_{t-s}^{-1}$$\n",
    "\n",
    "Where $\\tau_V < \\tau_C$ and the $\\kappa$ terms are normalizing terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc4ce8-4c89-4161-b13f-c1cfbf89e6d7",
   "metadata": {},
   "source": [
    "### Short-Term Volatility Updating (STVU)\n",
    "\n",
    "This can be applied in addition to the dyanmic conditional correlation when we have estimated $V$ and $C$. Which aims at further adjusting the covariance matrix for more flexibility to sudden increase and decrease in volatility.\n",
    "\n",
    "$$f_t = e^{x_t/2} V_t C_t^{1/2} \\eta_t$$\n",
    "$$\\eta_t \\sim N(0, I_m)$$\n",
    "$$x_{t+1} = \\phi x_t + \\sigma \\gamma_t$$\n",
    "\n",
    "If $x_t = 0$ for all t, then it's just the regular factor cov. Set:\n",
    "\n",
    "$$u_t = C_t^{-1/2}V_t^{-1}f_t$$\n",
    "$$x_t = log||u_t||^2 - log||\\eta_t||^2$$\n",
    "\n",
    "Where $u_t$ is the whittened factor returns and $x_t$ is derived from $f_t = e^{x_t/2} V_t C_t^{1/2} \\eta_t$. Set:\n",
    "$$y_t = x_t + \\epsilon_t = x_t + \\left(log||\\eta_t||^2 - E(log||\\eta_t||^2)\\right)$$\n",
    "\n",
    "Combined with:\n",
    "$$x_{t+1} = \\phi x_t + \\sigma \\gamma_t$$\n",
    "\n",
    "It's a state-space model, and $x_t$ can be solved as:\n",
    "\n",
    "$$x_t = (1 - K) \\sum^{\\infty}_{s=0} K^s \\left(x_{t-s}\\right)$$\n",
    "$$x_t = (1 - K) \\sum^{\\infty}_{s=0} K^s \\left(log||u_{t-s}||^2 - log||\\eta_{t-s}||^2\\right)$$\n",
    "$$x_t = (1 - K) \\sum^{\\infty}_{s=0} K^s log||u_{t-s}||^2 - \n",
    "    (1 - K) \\sum^{\\infty}_{s=0} K^s log||\\eta_{t-s}||^2$$\n",
    "$$x_t = (1 - K) \\sum^{\\infty}_{s=0} K^s log||u_{t-s}||^2 - \n",
    "    E(log||\\eta_t||^2)$$\n",
    "$$x_t = (1 - K) \\sum^{\\infty}_{s=0} K^s \\left(log||u_{t-s}||^2 - E(log||\\eta_t||^2)\\right)$$\n",
    "\n",
    "Set $K = e^{-1/\\tau_0}$ where $\\tau$ is the half-life (tunable):\n",
    "\n",
    "$$x_t = (1 - e^{-1/\\tau_0}) \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \\left(log||u_{t-s}||^2 - E(log||\\eta_t||^2)\\right)$$\n",
    "$$e^{x_t/2} = exp\\left[\\frac{1 - e^{-1/\\tau_0}}{2} \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \\left(log||u_{t-s}||^2 - E(log||\\eta_t||^2)\\right)\\right]$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$E(log||\\eta_t||^2) = E(\\log m ||\\eta_t||^2 / m) = log(m) + E(log(||\\eta||^2 / m) \\simeq \\log{m}$$\n",
    "\n",
    "The last sim equal is because $\\eta$ has unit variance, and low of large number makes  $||\\eta_t||^2 / m \\to 1$ as $m \\to \\infty$:\n",
    "\n",
    "\\begin{align}\n",
    "e^{x_t/2} & \\simeq \\exp\\left[\\frac{1 - e^{-1/\\tau_0}}{2} \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \\left(\\log||u_{t-s}||^2 - \\log m\\right)\\right] \\\\ \n",
    "    & = \\exp\\left[(1 - e^{-1/\\tau_0}) \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \\left(\\log(||u_{t-s}||^2/m)^{1/2}\\right)\\right] \\\\ \n",
    "    & = \\exp\\left[(1 - e^{-1/\\tau_0}) \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \\left(\\log(||u_{t-s}||/\\sqrt{m})\\right)\\right] \\\\ \n",
    "    & \\simeq \\exp\\left[(1 - e^{-1/\\tau_0}) \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \\left(||u_{t-s}||/\\sqrt{m} - 1\\right)\\right] \\\\ \n",
    "    & = \\exp\\left[(1 - e^{-1/\\tau_0}) \\left(\\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} ||u_{t-s}||/\\sqrt{m}\\right) - 1\\right] \\\\ \n",
    "    & \\simeq \\exp\\left[\\log\\left((1 - e^{-1/\\tau_0}) \\left(\\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} ||u_{t-s}||/\\sqrt{m}\\right)\\right)\\right] \\\\ \n",
    "    & = (1 - e^{-1/\\tau_0}) \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} ||u_{t-s}||/\\sqrt{m}\n",
    "\\end{align}\n",
    "\n",
    "The second and third sim equalities are linear approximation of the log operator: $log(1+x) \\simeq x$ for some small $x$.\n",
    "\n",
    "Since $u$ is the whittened factor returns, and $||u||/\\sqrt(m)$ is the standard deviation:\n",
    "- When this std is 1, no adjustment to the factor returns\n",
    "- When this std is greater than 1, adjust factors upward\n",
    "- When this std is less than 1, adjust factors downward\n",
    "\n",
    "### Correcting for autocorrelation in Factor Returns\n",
    "\n",
    "Factor returns are slightly autocorrelated in the short term, so we add in the autocovariance into the cov matrix estimate:\n",
    "\n",
    "$$[C_l]_{i,j} = cov(f_{t,i}, f_{t-l, j})$$\n",
    "$$\\hat{\\Omega}_f = C_0 + \\frac{1}{2} \\sum^{l_{max}}_{l=1}(C_l + C_l^T)$$\n",
    "\n",
    "For a more advanced adjustment:\n",
    "\n",
    "$$\\hat{\\Omega}_f = C_0 + \\sum^{l_{max}}_{l=1}\\left(1 - \\frac{l}{1+l_{max}}\\right)(C_l + C_l^T)$$\n",
    "\n",
    "The intuition is: adjust the covariance matrix upward, if there is a positive autocorrelation, and vice versa.\n",
    "\n",
    "**WHY???**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90430a2-3a4a-48bc-bebe-c36caeeb4579",
   "metadata": {},
   "source": [
    "## Estimating Idio Cov Matrix\n",
    "\n",
    "### Exponential Weighting\n",
    "\n",
    "We have an error vector at each time, $\\epsilon_t$. Let $E \\in R^{n \\times T}$ by the collections of the error vectors over time, e.g. each column is an error vector at a time. To estimate the covariance matrix $\\hat{\\Omega}_{\\epsilon}$, we use an exponential moving average: \n",
    "\n",
    "$$\\hat{\\Omega}_{\\epsilon} = EWE^T$$\n",
    "\n",
    "Where the matrix $W$ is a positive definite diagonal matrix. For a simple averaging, $W=I_T$. But for exponential weighting, we set $[W]_{t,t} = \\kappa \\exp(-t/\\tau)$ where $\\tau$ is the half life and $\\kappa$ is the normalizing term such that $trace(W) = T$\n",
    "\n",
    "### Visual Inspection\n",
    "\n",
    "This matrix should be diagonal (no correlation between assets and across time), or at least sparse. But the reality is usually neither, so it helps to check the matrix visually as there might be some leftover covariance that can be explained by additional factors. For example, a stock may have different share classes that are highly correlated but not identicial, so a share class factor would help remove such covariance in the idio covariance matrix\n",
    "\n",
    "### Short-Term Idio Update\n",
    "\n",
    "This is similar to STVU, but since the idio matrix should be diagonal, the update is simplified.\n",
    "\n",
    "\\begin{align}\n",
    "e^{\\hat{x}_t/2} & = \\kappa_0 \\sum^{\\infty}_{s=0} \n",
    "    e^{-s/\\tau_0} \n",
    "    \\sqrt{\n",
    "        \\frac{\\sum_i a_{i,t-s}(\\epsilon_{i,t-s} / \\hat{\\sigma}_{i,t-s})^2}\n",
    "            {\\sum_i a_{i,t-s}}\n",
    "        }\\\\\n",
    "a_{i,t} &= \\begin{cases}\n",
    "    1 - |t-T_{earn,i}|/\\tau_{earn}, \\text{if } |t-T_{earn,i}| \\leq \\tau_{earn} \\\\\n",
    "    0, \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "$a_{i,t}$ is a tent-shaped variable that increases as we get close to the event and reduces back to 0 when we are outside of the event impact window $\\tau_{earn}$. The volatility adjustment $\\exp(\\hat{x}_t/2)$ is quite similar to STVU where $\\epsilon_{i,t}/\\hat{\\sigma}_{i,t}$ is the whitened noise (no error correlations). In addition to the exponential weighting from STVU, we add the $a_{i,t}$ adjustment for a weighted estimate of the idio variance at each timestamp, note that $a_{i,t}$ is assigned to each asset at each time stamp, earnings schedule is different for different firms. Since $a_{i,t}=0$ when the firm isn't close to any event, the overall adjuster $\\exp(\\hat{x}_t/2)$ is muted.\n",
    "\n",
    "$$ \\hat{\\sigma}_{i,t}^2 \\leftarrow \\left[(1-a_{i,t}) + a_{i,t}e^{\\hat{x}_t}\\right] \\hat{\\sigma}_{i,t}^2$$\n",
    "\n",
    "### Off-Diagonal Clustering\n",
    "\n",
    "Some assets might be highly correlated because a) same firm different share classses b) similarity between firms, VISA and MasterCard, etc. We can use a correlation threshold to identify the clusterings\n",
    "\n",
    "$$thres_{\\lambda}(\\rho_{i,j}) = \\rho_{i,j}1\\{|\\rho_{i,j} > \\lambda\\}$$\n",
    "$$\\lambda = K\\sqrt{\\log {n/T}}$$\n",
    "\n",
    "Basically, pick a positive constant, and the correlation threshold $\\lambda$ should be higher if we have more assets or shorter time horizon. \n",
    "\n",
    "### Matrix Shrinkage\n",
    "\n",
    "This is the same as factor cov matrix shrinkage, expcept that we don't need to deal with the autocovariance:\n",
    "\n",
    "$$ \\Omega_{\\epsilon, shrink}(\\rho) = (1-\\rho)\\hat{\\Omega}_{\\epsilon} + \\rho\\frac{trace(\\hat{\\Omega}_{\\epsilon})}{m}I_m$$\n",
    "\n",
    "We are just pulling the covariance matrix to the average of variances.\n",
    "\n",
    "## Winsorization of Returns\n",
    "\n",
    "Historical data can be wrong because: 1) data recording issue, wrong data, 2) truly outliers caused by real events. \n",
    "Use robust z-score to spot outliers:\n",
    "\n",
    "$$ d_{i,t} = \\frac{|\\log (1+r_{i,t})|}{\\text{median}(|\\log(1+r_{i,t-1})|,\\ldots,|\\log(1+r_{i,t-T})|}$$\n",
    "\n",
    "$T$ is the lookback window, and threshold is set somewhere between 5 and 10. Make sure to record, report and examine each of the outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ea131-f215-4aaa-8fff-3f271a5fc92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
