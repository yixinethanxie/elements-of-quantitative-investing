{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50571af2-6e6f-4c3b-9f9d-48b17303b54d",
   "metadata": {},
   "source": [
    "# Chapter 7 Appendix\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### 7.1\n",
    "Prove that a matrix A with shape (nxT) is of rank $m \\leq \\min \\{n, T\\}$ if and only if it can be decomposed into the product of two matrices B (nxm) and C (mxT) that are full rank.\n",
    "\n",
    "proof:\n",
    "\n",
    "Direction 1($\\Rightarrow$)\n",
    "\n",
    "Apply SVD on A: $A = USV^T$, since A has rank r, we can take the top r singular values and its corresponding columns in U an V, since the rest of the singular values are 0 and we can safely omit them. The matrix A now becomes $A = U_rS_rV_r^T$. Set $B=U_rS_r$ and $C=V_r^T$, we get the result $A=BC$.\n",
    "\n",
    "Direction 2($\\Leftarrow$)\n",
    "\n",
    "If $\\text{rank} (B) = \\text{rank} (C) = m$ then $\\text{rank} (A) = \\text{rank} (BC) = m$.\n",
    "\n",
    "### 7.2\n",
    "\n",
    "(7.7) solves for the first eigenvector (largest eigenvalue) and (7.10) solves for the second eigenvector (second largest eigenvalue). It is possible that a covariance matrix has repeated eigenvalues, and infinitely many eigenvectors, so I don't know how to prove the solution (eigenvector) uniqueness. However, the constraint being binding indicates that the lagrangian multiplier $\\lambda$ must be positive, which is also the eigenvalue. Again, since a covariance matrix must be PSD, there is at least one positive eigenvalue, so (7.7) must be binding, yet the second eigenvalue may be zero.\n",
    "\n",
    "### 7.3\n",
    "\n",
    "the objective can be translated as following, which is sum of the variances of the eigenportfolios.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{trace }\\left(W^T\\hat{\\Sigma}W\\right) = \\sum_{i=1}^{m} w_i^T\\hat{\\Sigma}w_i\n",
    "\\end{align}\n",
    "\n",
    "since each eigenvector $w_i$ must corresponding to the max eigenvalue while being orthogonal to all preceding eigenvectors, the sum of eigenvalues are maximized.\n",
    "the constratraint can be translated as following, which means that each weight vector must be unit norm and orthogonal to each other.\n",
    "\n",
    "\\begin{align}\n",
    "W^TW &= I_m \\\\\n",
    "w_i^Tw_i &= 1 \\\\\n",
    "w_i^Tw_j &= 0 \\text{, where } i \\neq j \\\\\n",
    "\\end{align}\n",
    "\n",
    "since the weight constraint must be binding from the previous proof, and eigenvectors are all orthogonal to each other, the constraints match.\n",
    "\n",
    "### 7.4\n",
    "\n",
    "\\begin{align}\n",
    "x &= Ar \\\\\n",
    "xx^T &= Arr^TA^T \\\\\n",
    "&= A\\Sigma A^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "### 7.5\n",
    "\n",
    "\\begin{align}\n",
    "BB^T + \\sigma^2I_n &= U\\Lambda U^T + U\\text{diag}(\\sigma_i^2)U^T \\\\\n",
    "&= U\\text{diag}(\\lambda_i + \\sigma_i^2)U^T\n",
    "\\end{align}\n",
    "\n",
    "The first equal is because $U$ is orthogonal thus $UU^T = I_n$. Since $BB^T$ is PD (strictly positive because $B$ is full-rank), therefore, all its eigenvalues are strictly positive and the combo's eigenvalues are all greater than $\\sigma^2$.\n",
    "\n",
    "### 7.6\n",
    "\n",
    "The objective is to get the estimated covariance matrix as close to the empirical one as possible. Without the constraints, we can just equate them. With the constraint that the esimated covariance matrix has lower rank, we can apply PPCA on the empirical covariance matrix and select the top m (matching $B$) eigenvectors, so\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\Sigma}_r = \\hat{B}\\hat{B}^T + \\hat{\\sigma}I_n &\\approx \\Sigma_r = USU^T \\approx U_mS_mU_m^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "plug in the solutions \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{B} &= U_m(S_m^2 - \\hat{\\sigma}^2I_n)^{1/2} \\\\\n",
    "\\hat{\\sigma}^2 &= \\bar{\\lambda} \\\\\n",
    "\\hat{B}\\hat{B}^T + \\hat{\\sigma}^2I_n &= U_m(S_m - \\hat{\\sigma}^2I_n)U_m^T + \\hat{\\sigma}^2I_n = U_mS_mU_m^T\n",
    "\\end{align}\n",
    "\n",
    "Set $\\sigma^2$ as the average of last $n-m$'s eigenvalues to offset the idio covariances.\n",
    "\n",
    "### 7.7 \n",
    "\n",
    "The power method iteratively updates $x_{i+1} \\leftarrow \\Sigma x_{i}$ and normalize it to norm-1 $x_{i+1} \\leftarrow x_{i+1} / ||x_{i+1}||$. Therefore, $x_{i} = \\Sigma^{i}x_0 / ||\\Sigma^{i}x_0||$. Let $v_i$ and $\\lambda_i$ be the ith eigenvector and eigenvalue, then $x_0 = \\sum_{i=1}a_iv_i$ for some real $a_i$ as $V$ span the entire vector space. \n",
    "\\begin{align}\n",
    "\\Sigma^i x_0 &= \\sum_{j=1}a_j\\Sigma^iv_j \\\\\n",
    "            &= \\sum_{j=1}a_j\\lambda_j^iv_j \\\\\n",
    "            &= a_1\\lambda_1^iv_1 + \\sum_{j=2}a_j\\lambda_j^iv_j \\\\\n",
    "            &= \\lambda_1^i\\left(a_1v_1 + \\sum_{j=2}a_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^iv_j\\right) \\\\\n",
    "            &\\rightarrow \\lambda_1^ia_1v_1\n",
    "\\end{align}\n",
    "\n",
    "The last line is because $\\lambda_1 > \\lambda_2 \\geq \\ldots \\geq \\lambda_n$ and the ratio is less than 1 and approaches 0 as i increases. We assume that $\\lambda_1$ is unique, otherwise the algorithm doesn't work as there are infenitely many eigenvectors corresponding to the largest eigenvalue.\n",
    "\n",
    "\\begin{align}\n",
    "x_{i} &= \\frac{\\Sigma^ix_0}{||\\Sigma^ix_0||} \\\\\n",
    "            &\\rightarrow \\frac{\\lambda_1^ia_1v_1}{||\\lambda_1^ia_1v_1||} \\\\\n",
    "            &= \\text{sgn}(a_1)v_1\n",
    "\\end{align}\n",
    "\n",
    "Therefore, we proved that $x_{i}$ is asymptotically collinear with $v_{1}$ and $x_{i}^T\\Sigma x_{i} \\to \\lambda_1$\n",
    "\n",
    "I didn't manage to solve the second part, but the idea is to measure how fast $x_i$ converges to $v_1$, which is related to $\\lambda_2 / \\lambda_1$, the smaller it is the faster the \"error\" term shrinks and the faster the convergence.\n",
    "\n",
    "Finally, to extend this algorithm to find all eignvectors, we can peel the covariance matrix by the found eigenvectors and eigenvalues. For example, to find the second eigenvector, we transform the covariance matrix $\\Sigma \\leftarrow \\Sigma - (x_1^T\\Sigma x_1)x_1x_1^T$.\n",
    "\n",
    "OR we can keep covariance as is, but orthogonalize $x_2$ at each iteration. $x_2 \\leftarrow x_2 - (x_1x_1^T)x_2$. Note that $x_1x_1^T$ is the hat matrix that projects $x_2$ onto the subspace spanned by $x_1$, removing the part of $x_2$ spanned by $x_1$ orthogonalize $x_2$.\n",
    "\n",
    "### 7.8\n",
    "\n",
    "Applying power method to solve SVD is similar, the update rule can be converted to\n",
    "\\begin{align}\n",
    "x_{i+1} &= R^TRx_i \\\\\n",
    "y_{i+1} &= RR^Ty_i\n",
    "\\end{align}\n",
    "The method to find the left eigenvector $x_i$ is to apply power method to $R^TR$, similarly $y_i$ by $RR^T$, finally, the singular value is $y_{i}^TRx_i$. The proof for convergence and correctness is similar to 7.7.\n",
    "\n",
    "To extend it to solve the entire SVD, the idea is the same, peeling $R^TR$ by each left eigenvectors and $RR^T$ by each right eigenvectors. \n",
    "\n",
    "### 7.9\n",
    "\n",
    "\\begin{align}\n",
    "R &= USV^T = B\\hat{F} + \\epsilon = BS_mV_m^T + \\epsilon \\\\\n",
    "B &= R(F^TF)^{-1}F^T \\\\\n",
    "&= R(V_mS_mS_mV_m^T)^{-1}V_mS_m \\\\\n",
    "&= RV_mS_m^{-2}V_m^TV_mS_m \\\\\n",
    "&= USV^TV_mS_m^{-2}V_m^TV_mS_m \\\\\n",
    "&= U_m\n",
    "\\end{align}\n",
    "\n",
    "### 7.10\n",
    "\n",
    "The essence is that we need to fine a unit-vector $v$ such that $v^TXv$ is maximized, which makes the maximum value the first eigenvalue and $v$ the first eigenvector.\n",
    "\n",
    "Combining the objective function and the unit-vector constraint, we have\n",
    "\n",
    "\\begin{align}\n",
    "f &= (v^TXv) / ||v||^2 - \\lambda ||v||^2 \\\\\n",
    "\\nabla_vf &= 2Xv - 2(v^TXv)v = 2(1 - vv^T)Xv\n",
    "\\end{align}\n",
    "\n",
    "The second line is because $||v||^2 = 1$, given the gradient, we can use stochastic gradient descent to find the optimum, thus the update rule in the algorithm.\n",
    "\n",
    "### 7.11\n",
    "\n",
    "The goal is to find the min distance between the subspace as the distance where $x$ and $y$ are both unit-vector:\n",
    "\n",
    "\\begin{align}\n",
    "S(A,B) &= \\frac{1}{2} \\min_{x,y} ||Ax - By||^2 \\\\\n",
    "    &= \\min \\frac{1}{2} (Ax - By)^T(Ax - By) \\\\\n",
    "    &= \\min \\frac{1}{2}\\left(x^TA^TAx + y^TB^TBy - 2x^TA^TBy\\right) \\\\\n",
    "    &= 1 - \\max x^TA^TBy \\\\\n",
    "    &= 1 - \\sigma_1(A^TB) \n",
    "\\end{align}\n",
    "\n",
    "The last equal sign is because the max is reached at the first singular value.\n",
    "\n",
    "This is not a distance\n",
    "\\begin{align}\n",
    "A &= (1, 0)^T \\\\\n",
    "B &= 1/\\sqrt{2} (1, 1)^T \\\\\n",
    "C &= (0, 1)^T \\\\\n",
    "S(A,B) &= 1 - \\sigma_1(A^TB) = 1 - 1/\\sqrt{2} \\\\\n",
    "S(A,C) &= 1 - 0 = 1 \\\\\n",
    "S(B,C) &= 1 - 1/\\sqrt{2} \\\\\n",
    "S(A,C) &= 1 > S(A,B) + S(B,C) = 1 - 1/\\sqrt{2} + 1 - 1/\\sqrt{2} = 2 - \\sqrt{2}\n",
    "\\end{align}\n",
    "The last inequality counter proves the triangle inequality\n",
    "\n",
    "### 7.12\n",
    "\n",
    "The cosine distance between two subspaces is:\n",
    "\n",
    "\\begin{align}\n",
    "S_C(A, B) &= \\min_{x, y} x^TA^TBy \\\\\n",
    "&= \\sigma_m(A^TB)\n",
    "\\end{align}\n",
    "\n",
    "The last equality follows the same logic as 7.11, min is achieved with the last signular value.\n",
    "\n",
    "### 7.13\n",
    "\n",
    "part 1:\n",
    "\n",
    "\\begin{align}\n",
    "r &= Bf + \\epsilon \\\\\n",
    "\\Omega_r &= B\\Omega_fB^T \\\\\n",
    "&= \\bar{\\lambda}DUU^TD \\\\\n",
    "&= \\bar{\\lambda}DUQQ^TU^TD \\\\\n",
    "&= \\bar{\\lambda}D\\tilde{U}\\tilde{U}^TD\n",
    "\\end{align}\n",
    "\n",
    "where $Q^TQ=I$ and $col(U) = col(UQ) = col(\\tilde{U})$.\n",
    "\n",
    "part 2:\n",
    "\n",
    "\\begin{align}\n",
    "\\Omega_r &= B\\Omega_fB^T \\\\\n",
    "&= B_1\\Omega_{f1}B_1^T + B_2\\Omega_{f2}B_2^T \\\\\n",
    "&= D_1U_1\\Omega_{f1}U_1^TD_1^T + D_2U_2\\Omega_{f2}U_2^TD_2^T \\\\\n",
    "&= D_1U_1\\Omega_{f1}U_1^TD_1^T + \\lambda_mD_2U_2U_2^TD_2^T \\\\\n",
    "&= D_1\\tilde{U}_1\\Omega_{f1}\\tilde{U}_1^TD_1^T + \\lambda_mD_2\\tilde{U}_2\\tilde{U}_2^TD_2^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "the last equal sign follows the same logic as part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c48eb-a28a-4a15-b7a3-7bd5619e675f",
   "metadata": {},
   "source": [
    "## Asymptotic Properties of PCA\n",
    "\n",
    "As the dimension $n$ is fixed and the sample size $T$ increases, the CLT on covariance matrix's eigenvalues and eigenvectors are as follows:\n",
    "\n",
    "- The standard error on $\\hat{\\lambda}_i = 2\\lambda_i / \\sqrt{T}$.\n",
    "- The standard error on the eigenvectors:\n",
    "  $$\\sqrt{E(||\\hat{u}_i - u_i||^2)} = \\frac{1}{\\sqrt{T}}\\sqrt{\\sum^n_{k=1,k\\neq i} \\frac{\\lambda_k\\lambda_i}{(\\lambda_k - \\lambda_i)^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31837ab4-4e96-468b-9b92-7cb8fad353cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
