{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9003acb5-d5ef-430f-8698-14dc502a0e92",
   "metadata": {},
   "source": [
    "# Chapter 7 - Statistial Factor Models\n",
    "\n",
    "## 7.1 Basics\n",
    "\n",
    "### 7.1.1 Best Low-Rank Approximation and PCA\n",
    "\n",
    "A factor model is $R = BF$, to decompose the returns $R$ into low-ranked factors $F$ via factor loadings $B$, we run SVD, $R=USV^T$. Sort the columns of $U$ and rows of $V$ based on the diagonal matrix $S$ in descending order, we can select the first $m$ components where $m<n$, such that\n",
    "\n",
    "$$R \\simeq U_mS_mV_m^T $$\n",
    "\n",
    "Assign\n",
    "\n",
    "$$B = U_m$$\n",
    "$$F = S_mV_m^T$$\n",
    "\n",
    "To formulate PCA as a eigenvalue decomposition (EVD) problem, we have\n",
    "\n",
    "$$ \\max w^T\\hat{\\Sigma}w$$\n",
    "$$\\text{s.t. } ||w|| \\leq 1$$\n",
    "\n",
    "Solve it with Langrangian\n",
    "\n",
    "\\begin{align}\n",
    "L & = -w^T\\hat{\\Sigma}w + \\lambda (w^Tw - 1) \\\\\n",
    "dL & = -2\\hat{\\Sigma}w + 2\\lambda w = 0 \\\\\n",
    "    \\hat{\\Sigma}w &= \\lambda w \\\\\n",
    "    \\lambda &= w^T\\hat{\\Sigma}w\n",
    "\\end{align}\n",
    "\n",
    "Since the goal is to maximize $w^T\\hat{\\Sigma}w$ and $\\lambda = w^T\\hat{\\Sigma}w$, and solution is the eignvector associated with the highest eignvalue. \n",
    "\n",
    "To formulate PCA as a SVD problem, we have \n",
    "\\begin{align}\n",
    "R &= USV^T \\\\\n",
    "\\hat{\\Sigma} &= \\frac{1}{T} RR^T \\\\\n",
    "            &= \\frac{1}{T} (USV^T)(VSU^T) \\\\\n",
    "            &= \\frac{1}{T} US^2U^T \\\\\n",
    "w^T\\hat{\\Sigma}w &= \\frac{1}{T} w^TUS^2U^Tw \\\\\n",
    "\\end{align}\n",
    "\n",
    "Set $U^Tw = v$, we have\n",
    "\\begin{align}\n",
    "\\text{max } & v^TS^2v \\\\\n",
    "\\text{s.t. } & v^Tv \\leq 1\n",
    "\\end{align}\n",
    "\n",
    "Since $S^2$ is a diagonal matrix sorted in descending order, the optimal solution is $v = (1,0,\\ldots,0)^T$, e.g. we want to select the first (largest) diagonal value.\n",
    "\n",
    "If the diagonal values (eigen values) in $S^2$ aren't unique, the solution $v$ isn't unique either, say we have $\\lambda_1 = \\lambda_2$:\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_1 &= v_1^TS^2v_1 \\\\\n",
    "\\lambda_1 &= v_1^2S_1^2 \\\\\n",
    "\\lambda_2 &= v_2^TS^2v_2 \\\\\n",
    "\\lambda_2 &= v_2^2S_2^2\n",
    "\\end{align}\n",
    "\n",
    "Then there is $\\lambda_3 = \\lambda_1 + \\lambda_2$ with eignvector $v_3 = (v_1, v_2, \\ldots,0)^T$ that\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_3 &= v_1^2S_1^2 + v_2^2S_2^2 = v_3^TS^2v_3\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Finally, to solve for $m < n$ components in one go, we have\n",
    "\n",
    "\\begin{align}\n",
    "\\text{max } & trace(W^T\\hat{\\Sigma}W) \\\\\n",
    "\\text{s.t. } & W^TW = I_m \\\\\n",
    "W & \\in R^{n\\times m}\n",
    "\\end{align}\n",
    "\n",
    "Same as solving eignvectors one by one with each subsequent problem having an additional contraint that the new eigenvector being orthogonal to the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02f5a5-e274-49d7-8ef8-e804e1ceacc5",
   "metadata": {},
   "source": [
    "### 7.1.2 MLE and PCA\n",
    "\n",
    "The idea is to start with a rotated factor model so that the factor covariance matrix is an identity matrix, which can be achieved following section 4.4.1 (identify factor covariance matrix). Assume constant and diagonal idio covariance matrix. \n",
    "\n",
    "$$ f \\sim N(0,I_m) $$\n",
    "$$ \\epsilon \\sim N(0, \\sigma^2 I_n) $$\n",
    "$$ \\Sigma_r = BB^T + \\sigma^2 I_n $$\n",
    "\n",
    "MLE on the factor covariance matrix becomes:\n",
    "\n",
    "$$ \\max -\\log|\\hat{\\Sigma}_r| - \\langle \\hat{\\Sigma}_r^{-1}, \\Sigma_r \\rangle $$\n",
    "$$ \\hat{\\Sigma}_r = \\hat{B}\\hat{B}^T + \\hat{\\sigma}^2I_n$$\n",
    "\n",
    "Where $\\Sigma_r$ is the empirical covariance matrix. The solution is:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{B} & = U_m(S_m - \\hat{\\sigma}I_m)^{1/2} \\\\\n",
    "    \\hat{\\sigma}^2 & = \\bar{\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "Where $\\bar{\\lambda}$ is the average of the last n-m eigenvalues. We can apply a transformation to make it more intuitive:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{B} & = U_m \\\\\n",
    "    \\Sigma_f & = S_m - \\bar{\\lambda}I_m \\\\\n",
    "    \\Sigma_{\\epsilon} & = \\bar{\\lambda}I_n \n",
    "\\end{align}\n",
    "\n",
    "The transformation provides the insight: we shrink factor variance by the \"unselected\" factors and categorize those as idio returns/variance. \n",
    "\n",
    "Experiments show that:\n",
    "- shrinkage mentioned above help counters the upward bias of sample factor variance estimate.\n",
    "- however, the shrinkage tends to overcorrete and introduce downward bias. Therefore, the MLE approach is generally biased.\n",
    "\n",
    "\n",
    "### 7.1.3 Regressions via SVD\n",
    "\n",
    "Once we know the factor loadings $U_m$ and asset returns $R$, we can get the factor returns via regression and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89487d81-e5f9-402c-8be5-55d88ce5f6ad",
   "metadata": {},
   "source": [
    "## 7.2 Beyond Basics\n",
    "\n",
    "### Spiked Covariance Model\n",
    "\n",
    "For the empirical covariance matrix:\n",
    "$$\\tilde{\\Omega}_r = T^{-1}\\sum^T_{t=1}r_tr_t^T$$\n",
    "\n",
    "The matrix is spiked if there is an $m$ that $0<m<n$ and a positive constant $C$ such that as $T \\to \\infty$, the eigenvalues:\n",
    "\n",
    "$$\\lambda_i = \\lim_{T\\to\\infty}\\lambda_{T,i}\\begin{cases}\n",
    "= 1 \\text{    for all i > m} \\\\\n",
    "\\geq Cn \\text{    for all i } \\leq m\n",
    "\\end{cases}$$\n",
    "\n",
    "A spiked matrix has a subset of eigenvalues that scale with the size of the matrix, while the rest of the eignvalues is constant. In the context of factor model, as we add more assets into the universe, the variance between factor and idio returns become larger and larger, to a point that the idio variances are negligible thus \"diversified\" away.\n",
    "\n",
    "To see this, assume we have done the rotation and rescaling that both factor and idio covariance matrices are identiy matrix, and the covariance matrix is now: $BB^T+I_n$. Since the eignvalues of $BB^T$ and $B^TB$ are the same, we can write $B^TB = \\sum b_i^Tb_i$ where $b_i$ is ith row of $B$. We can then rewrite the $B^TB = n(1/n\\sum b_i^Tb_i) = nE(b_i^Tb)$. Let $\\mu_i$ be the eigvalues of the expectation, the eigenvalues of $B^TB$ are then $n\\mu_i$. The overall covariance matrix is now $n\\mu_i + 1$. As n increases the factor eignvalues increase while the idio eigenvalues stay constant.\n",
    "\n",
    "To illustrate and borrow points from section 9.3, the ith FMP is $w_i = B(B^TB)^{-1}e_i$. Plug into the covariance matrix. Factor covariance matrix:\n",
    "\n",
    "$$w_i^T(BB^T)w_i = e_i^{T}(B^TB)^{-1}B^T(BB^T)B(B^TB)^{-1}e_i=1$$\n",
    "\n",
    "Idio covariance matrix:\n",
    "\n",
    "\\begin{align}\n",
    "w_i^Tw_i &= e_i^T(B^TB)^{-1}B^TB(B^TB)^{-1}e_i \\\\ \n",
    "        &= e_i^T(B^TB)^{-1}e_i \\\\\n",
    "        &= e_i^TVS^{-2}V^Te_i \\\\\n",
    "        &\\leq \\lambda_m^{-1} e_i^TVV^Te_i \\\\\n",
    "        &= \\lambda_m^{-1} ||V^Te_i||^2 \\\\\n",
    "        &\\leq \\lambda_m^{-1} ||V^T||^2||e_i||^2 \\\\\n",
    "        &= \\lambda_m^{-1} ||V^T||^2 \\\\\n",
    "        &= \\lambda_m^{-1} \\\\\n",
    "        &\\leq 1/Cn\n",
    "\\end{align}\n",
    "Where $\\lambda_m$ is the mth largest eignvalue of $BB^T$ and $B^TB$\n",
    "\n",
    "### Spectral Limit Behavior of the Spiked Covariance Model\n",
    "\n",
    "Let $\\gamma = n/T$, assume:\n",
    "\n",
    "1. The elements of $r_t$ have finite fourth moments\n",
    "2. As $n,T \\to \\infty$, for $i=1,\\ldots,m$\n",
    "   $$\\frac{\\gamma}{\\lambda_i} \\to c_i$$\n",
    "4. The remaining n-m eigenvalues are equal to one. \n",
    "\n",
    "Then:\n",
    "1. When $\\lambda_i > 1 + \\sqrt{\\gamma}$:\n",
    "   $$\\hat{\\lambda}_i \\to \\lambda_i\\left(1 + \\frac{\\gamma}{\\lambda_i}\\right) = \\lambda_i + \\gamma$$\n",
    "   $$|\\langle u_i, \\hat{u}_i \\rangle| \\to \\begin{cases}\n",
    "       \\frac{1}{\\sqrt{1+c_i}}, i \\leq m \\\\\n",
    "       O(1/\\sqrt{\\gamma}), i > m\n",
    "   \\end{cases}\n",
    "   $$\n",
    "2. When $\\lambda_i \\leq 1 + \\sqrt{\\gamma}$:\n",
    "   $$\\hat{\\lambda}_i \\to \\left(1 + \\sqrt{\\gamma}\\right)^2$$\n",
    "   $$|\\langle u_i, \\hat{u}_i \\rangle| \\to 0$$\n",
    "\n",
    "So, translate to plain English:\n",
    "1. When the true eigenvalue is large, the sample eigenvalue is upward biased, while asymptotically unbiased with large true eignvalue. The larger the true eigenvalue the more accurate the sample eigenvalue.\n",
    "2. When the true eigenvalue is large, for the largest m eigenvalues' eigenvectors, the true and sample eigenvectors are more similar if the eignvalues are large or the n/T ratio is small (more data points). While for the rest of the eigenvectors, the similarity is only relevant to the n/T ratio, eigenvalues are ones anyways.\n",
    "3. When the true eigenvalue is small, the sample eigenvalue converges to a constant, and there are close to 0 similarity between true and sample eigenvectors. Basically, if the eigenvector is small, they are just noise.\n",
    "\n",
    "To summarize a bit more, the helpful case is when the first m true eigenvalues are large, the larger the better and the more data point the better. In practice the threshold $1+\\sqrt{\\gamma}$ is somewhere between 2 and 7.\n",
    "\n",
    "### Optimal Shrinkage of Eigenvalues\n",
    "\n",
    "Given the spectral limit behavior above, we can correct the bias by reversing it\n",
    "$$l(\\lambda) = \\lambda-\\gamma, \\lambda \\geq 1 + \\sqrt{\\gamma}$$\n",
    "\n",
    "The experiments show that the above shrinkage works well with normal returns but under shrink the fat tailed (t-student) returns. A more complete shrinkage may be:\n",
    "\\begin{align}\n",
    "l(\\lambda) &= \\kappa_1 \\lambda-\\kappa_2 \\\\ \n",
    "    \\kappa_2 &\\geq \\lambda_{min} \\\\\n",
    "    \\kappa_1 &\\in (0,1)\n",
    "\\end{align}\n",
    "\n",
    "### Choosing the number of factors\n",
    "\n",
    "1. choosing the right factors matter more than choosing the right number of factors\n",
    "2. it's either very obvious or not at all\n",
    "3. when in doubt, always over-select factors\n",
    "\n",
    "Threshold-based methods, based on the spectral limit behavior, select the first m factors greater than $1+\\sqrt{\\gamma}$. It should be obvious that there is a gap between the first m factors and the rest.\n",
    "\n",
    "Maximum change point, sort the eigenvalues and find the ones with the most change between consecutively ranked eigenvalues.\n",
    "\n",
    "Penalty-based method, the idea is to select the minimum amount of factors that minimize the error between true returns and estimated returns. \n",
    "$$ \\min_{k} || R-\\hat{R}||^2 + kf(n,T) $$\n",
    "$$ f(n,T) = \\frac{n+T}{nT}\\log\\left(\\frac{nT}{n+T}\\right)$$\n",
    "\n",
    "Without the penalty term, all factors will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7349ae8-aa42-484c-bbac-b57d71c87245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
