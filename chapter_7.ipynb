{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9003acb5-d5ef-430f-8698-14dc502a0e92",
   "metadata": {},
   "source": [
    "# Chapter 7 - Statistial Factor Models\n",
    "\n",
    "## 7.1 Basics\n",
    "\n",
    "### 7.1.1 Best Low-Rank Approximation and PCA\n",
    "\n",
    "A factor model is $R = BF$, to decompose the returns $R$ into low-ranked factors $F$ via factor loadings $B$, we run SVD, $R=USV^T$. Sort the columns of $U$ and rows of $V$ based on the diagonal matrix $S$ in descending order, we can select the first $m$ components where $m<n$, such that\n",
    "\n",
    "$$R \\simeq U_mS_mV_m^T $$\n",
    "\n",
    "Assign\n",
    "\n",
    "$$B = U_m$$\n",
    "$$F = S_mV_m^T$$\n",
    "\n",
    "To formulate PCA as a eigenvalue decomposition (EVD) problem, we have\n",
    "\n",
    "$$ \\max w^T\\hat{\\Sigma}w$$\n",
    "$$\\text{s.t. } ||w|| \\leq 1$$\n",
    "\n",
    "Solve it with Langrangian\n",
    "\n",
    "\\begin{align}\n",
    "L & = -w^T\\hat{\\Sigma}w + \\lambda (w^Tw - 1) \\\\\n",
    "dL & = -2\\hat{\\Sigma}w + 2\\lambda w = 0 \\\\\n",
    "    \\hat{\\Sigma}w &= \\lambda w \\\\\n",
    "    \\lambda &= w^T\\hat{\\Sigma}w\n",
    "\\end{align}\n",
    "\n",
    "Since the goal is to maximize $w^T\\hat{\\Sigma}w$ and $\\lambda = w^T\\hat{\\Sigma}w$, and solution is the eignvector associated with the highest eignvalue. \n",
    "\n",
    "To formulate PCA as a SVD problem, we have \n",
    "\\begin{align}\n",
    "R &= USV^T \\\\\n",
    "\\hat{\\Sigma} &= \\frac{1}{T} RR^T \\\\\n",
    "            &= \\frac{1}{T} (USV^T)(VSU^T) \\\\\n",
    "            &= \\frac{1}{T} US^2U^T \\\\\n",
    "w^T\\hat{\\Sigma}w &= \\frac{1}{T} w^TUS^2U^Tw \\\\\n",
    "\\end{align}\n",
    "\n",
    "Set $U^Tw = v$, we have\n",
    "\\begin{align}\n",
    "\\text{max } & v^TS^2v \\\\\n",
    "\\text{s.t. } & v^Tv \\leq 1\n",
    "\\end{align}\n",
    "\n",
    "Since $S^2$ is a diagonal matrix sorted in descending order, the optimal solution is $v = (1,0,\\ldots,0)^T$, e.g. we want to select the first (largest) diagonal value.\n",
    "\n",
    "If the diagonal values (eigen values) in $S^2$ aren't unique, the solution $v$ isn't unique either, say we have $\\lambda_1 = \\lambda_2$:\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_1 &= v_1^TS^2v_1 \\\\\n",
    "\\lambda_1 &= v_1^2S_1^2 \\\\\n",
    "\\lambda_2 &= v_2^TS^2v_2 \\\\\n",
    "\\lambda_2 &= v_2^2S_2^2\n",
    "\\end{align}\n",
    "\n",
    "Then there is $\\lambda_3 = \\lambda_1 + \\lambda_2$ with eignvector $v_3 = (v_1, v_2, \\ldots,0)^T$ that\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_3 &= v_1^2S_1^2 + v_2^2S_2^2 = v_3^TS^2v_3\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Finally, to solve for $m < n$ components in one go, we have\n",
    "\n",
    "\\begin{align}\n",
    "\\text{max } & trace(W^T\\hat{\\Sigma}W) \\\\\n",
    "\\text{s.t. } & W^TW = I_m \\\\\n",
    "W & \\in R^{n\\times m}\n",
    "\\end{align}\n",
    "\n",
    "Same as solving eignvectors one by one with each subsequent problem having an additional contraint that the new eigenvector being orthogonal to the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02f5a5-e274-49d7-8ef8-e804e1ceacc5",
   "metadata": {},
   "source": [
    "### 7.1.2 MLE and PCA\n",
    "\n",
    "The idea is to start with a rotated factor model so that the factor covariance matrix is an identity matrix, which can be achieved following section 4.4.1 (identify factor covariance matrix). Assume constant and diagonal idio covariance matrix. \n",
    "\n",
    "$$ f \\sim N(0,I_m) $$\n",
    "$$ \\epsilon \\sim N(0, \\sigma^2 I_n) $$\n",
    "$$ \\Sigma_r = BB^T + \\sigma^2 I_n $$\n",
    "\n",
    "MLE on the factor covariance matrix becomes:\n",
    "\n",
    "$$ \\max -\\log|\\hat{\\Sigma}_r| - \\langle \\hat{\\Sigma}_r^{-1}, \\Sigma_r \\rangle $$\n",
    "$$ \\hat{\\Sigma}_r = \\hat{B}\\hat{B}^T + \\hat{\\sigma}^2I_n$$\n",
    "\n",
    "Where $\\Sigma_r$ is the empirical covariance matrix. The solution is:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{B} & = U_m(S_m - \\hat{\\sigma}I_m)^{1/2} \\\\\n",
    "    \\hat{\\sigma}^2 & = \\bar{\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "Where $\\bar{\\lambda}$ is the average of the last n-m eigenvalues. We can apply a transformation to make it more intuitive:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{B} & = U_m \\\\\n",
    "    \\Sigma_f & = S_m - \\bar{\\lambda}I_m \\\\\n",
    "    \\Sigma_{\\epsilon} & = \\bar{\\lambda}I_n \n",
    "\\end{align}\n",
    "\n",
    "The transformation provides the insight: we shrink factor variance by the \"unselected\" factors and categorize those as idio returns/variance. \n",
    "\n",
    "Experiments show that:\n",
    "- shrinkage mentioned above help counters the upward bias of sample factor variance estimate.\n",
    "- however, the shrinkage tends to overcorrete and introduce downward bias. Therefore, the MLE approach is generally biased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83402e-868a-438b-ba41-56f2c50fa415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
